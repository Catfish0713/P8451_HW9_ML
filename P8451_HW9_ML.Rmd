---
title: "P8451_HW9"
author: "Ruixi Li"
date: "2024-03-20"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data Preparation

```{r data_prep}
library(lattice)
library(NHANES)
library(dplyr)
library(caret)
library(randomForest)
library(pROC)


data ("NHANES")
table(NHANES$Diabetes)# the data is strongly imbalanced

keep.var<-names(NHANES) %in% c("Age", "Race1", "Education", "Poverty", "Weight", "Height", "Pulse", "Diabetes", "BMI", "PhysActive", "Smoke100", "BPSysAve", "BPDiaAve", "TotChol")

NHANES.subset<-NHANES[keep.var]

# check the coding and refernce group of the outcome
contrasts(NHANES.subset$Diabetes)
# Since the reference group is assigned correctly, I don't need to change it.

# Set up the reference group for the prediction outcome
# NHANES.subset = NHANES.subset |> mutate(Diabetes = relevel(Diabetes,ref = "No"))

skimr::skim(NHANES.subset) # all variables are numeric or factor, have missing

#Remove missings and then remove duplicates
NHANES.subset<-na.omit(NHANES.subset)
NHANES.subset<-unique(NHANES.subset)

#Check distributions
skimr::skim(NHANES.subset)


```

### Set up: Partition data into training/testing

```{r partition}

set.seed(123)

train.indices <- NHANES.subset %>%
  pull(Diabetes) %>%
  createDataPartition(p = 0.7, list = FALSE)

train.data <- NHANES.subset %>%
  slice(train.indices)

test.data <- NHANES.subset %>%
  slice(-train.indices)



```

### Model 1: Random Forest with 3 values of mtry and 3 values of ntree

```{r}

control.obj<-trainControl(method="cv", 
                          number=5, 
                          sampling="up", 
                          summaryFunction = twoClassSummary,
                          classProbs = TRUE)
#I use 5-fold cv here to reduce computational load
# The twoClassSummary function is designed for binary classification problems and will provide metrics like sensitivity, specificity, and Area Under the ROC Curve (AUC).
# ClassProbs=TRUE is necessary for twoClassSummary to work because it requires class probabilities to calculate AUC.

# hyperparameter tuning
# Try mtry of all, half of all, sqrt of all, 
# Try ntree of 100, 300, 500
feat.count<-c((ncol(train.data)-1), (ncol(train.data)-1)/2, sqrt(ncol(train.data)-1),2,3)
# I want to try more mtry, but it takes too long. Since the model have poor performace when mtry=1, I just add 2, 3 as candidate mtry.

grid.rf<-expand.grid(mtry=feat.count)

tree.num<-seq(100,500, by=200)

results.trees<-list()
for (ntree in tree.num){
  set.seed(123)
    rf.nhanes<-train(
                      Diabetes~., 
                      data=train.data, 
                      method="rf", 
                      trControl=control.obj, 
                      metric="ROC", 
                      tuneGrid=grid.rf, 
                      importance=TRUE, # the model will calculate variable importance measures
                      ntree=ntree)
    index<-toString(ntree)
  results.trees[[index]]<-rf.nhanes$results
}

plot(rf.nhanes)
output.nhanes<-bind_rows(results.trees, .id = "ntrees")
best.tune<-output.nhanes[which.max(output.nhanes[,"ROC"]),]
best.tune$mtry
# results.trees-no need to output
mtry.grid<-expand.grid(.mtry=best.tune$mtry)# choose the best tune to retrain the model

set.seed(123)
    rf.nhanes.bt<-train(
                      Diabetes~., 
                      data=train.data, 
                      method="rf", 
                      trControl=control.obj, 
                      metric="ROC", 
                      tuneGrid=grid.rf, 
                      importance=TRUE,
                      ntree=as.numeric(best.tune$ntrees))


varImp(rf.nhanes.bt)
varImpPlot(rf.nhanes.bt$finalModel)



```

* Random forest have the function of feature selection, we might consider focusing on the most important variables for a more parsimonious model or for insight into the features most strongly associated with the outcome variable. Given the variable importance plot, some Education, race category is not that important, but they can not be excluded seperately.

### Model 2: Support Vector Classifier

```{r}
set.seed(123)

control.obj<-trainControl(method="cv", 
                          number=5, 
                          sampling="up",
                          summaryFunction = twoClassSummary,
                          classProbs = TRUE)

#Repeat expanding the grid search
set.seed(123)

svc.nhanes<-train(
                  Diabetes ~ ., 
                  data=train.data, 
                  method="svmLinear", 
                  metric="ROC", 
                  trControl=control.obj, 
                  preProcess=c("center", "scale"), 
                  probability=TRUE, 
                  importance=TRUE,
                  tuneGrid=expand.grid(C=seq(0.0001,100, length=10)))
# for random forest, preProcess and probability is not necessary
svc.nhanes$results
plot(svc.nhanes)


```

* At very low cost values (near 0), the model performance is poorer, with ROC values significantly lower than the rest of the cost values. This indicates underfitting;The ROC performance increase sharply as cost increases from 0 to 10; Beyond a certain cost threshold (around 10 to 20), the ROC value levels off and becomes relatively stable despite further increases in cost. This suggests that the model has reached a point where adding more complexity (lower regularization) does not significantly improve cross-validated performance. There is some variability in ROC scores at higher cost values, but it does not show a clear trend of improvement or degradation. This slight variation is typical in cross-validation results due to the randomness inherent in the partitioning of data. I would say the best tune might be 11.1112. 


### Model 3: Logistic Regression
```{r}
set.seed(123)

control.obj<-trainControl(method="cv", 
                          number=5, 
                          sampling="up",
                          summaryFunction = twoClassSummary,
                          classProbs = TRUE)

logit.nhanes<-train(
                    Diabetes~., 
                    data=train.data, 
                    method="glm", 
                    family="binomial",
                    metric="ROC", 
                    preProcess=c("center", "scale"), 
                    trControl=control.obj)
# importance=TRUE is not applicable in logistic regression

logit.nhanes$results
coef(logit.nhanes$finalModel)
# plot(logit.nhanes$finalModel,select=3) 
# The plot() function for a glm object can produce several types of diagnostic plots to assess the fit of the model. The select argument specifies which type of plot to produce:

# select=1: Residuals vs Fitted
# select=2: Normal Q-Q
# select=3: Scale-Location (also known as Spread-Location or Standardized residuals vs. Fitted)
# select=4: Cook's Distance plot
# select=5: Residuals vs Leverage plot that helps us to find influential cases
```


# Using resamples(multiple evaluation methods) to compare the three models

```{r resamples}
res = resamples(list(RF = rf.nhanes.bt,
                     SVC = svc.nhanes,
                     GLM = logit.nhanes))
summary(res)
```

* I assume the purpose of this machine learning is to develop a tool to predict diabetes using above predictors. So, AUC value, which indicates the discriminative ability is important. Meanwhile, for most practical applications, a balance between sensitivity and specificity is desirable. Given the results, ROC of the three models are similar, RF's specificity is too low whereas its sensitivity is higher than the other two models. So RF is not the optimal model. SVC had higher sensitivity, AUC compared with Logistic regression ,their specificity are the same. So, I would choose SVC as the optimal mode. 


### Output predicted probabilities from each of the three models applied within the testing set. 

```{r}

#Predict in test-set and output probabilities
rf.probs<-predict(rf.nhanes, test.data, type="prob")

#Pull out predicted probabilities for Diabetes=Yes
rf.pp<-rf.probs[,2]


svc.probs<-predict(svc.nhanes,test.data, type="prob")
svc.pp<-svc.probs[,2]


#Predict in test-set using response type
logit.probs<-predict(logit.nhanes, test.data, type="prob")
logit.pp<-logit.probs[,2]

#Examine distributions of predicted probabilities
hist(rf.pp)
hist(svc.pp)
hist(logit.pp)
```

---
title: "P8451_HW9"
author: "Ruixi Li"
date: "2024-03-20"
output: word_document
---



# Data Preparation

```{r data_prep, warning=FALSE, message=FALSE}
library(lattice)
library(NHANES)
library(dplyr)
library(caret)
library(randomForest)
library(pROC)


data ("NHANES")
table(NHANES$Diabetes)# the data is strongly imbalanced

keep.var<-names(NHANES) %in% c("Age", "Race1", "Education", "Poverty", "Weight", "Height", "Pulse", "Diabetes", "BMI", "PhysActive", "Smoke100", "BPSysAve", "BPDiaAve", "TotChol")

NHANES.subset<-NHANES[keep.var]

# check the coding and refernce group of the outcome
contrasts(NHANES.subset$Diabetes)
# Since the reference group is assigned correctly, I don't need to change it.

# Set up the reference group for the prediction outcome
# NHANES.subset = NHANES.subset |> mutate(Diabetes = relevel(Diabetes,ref = "No"))

skimr::skim(NHANES.subset) # all variables are numeric or factor, have missing

#Remove missings and then remove duplicates
NHANES.subset<-na.omit(NHANES.subset)
NHANES.subset<-unique(NHANES.subset)

#Check distributions
skimr::skim(NHANES.subset)


```

# Set up: Partition data into training/testing

```{r partition}

set.seed(123)

train.indices <- NHANES.subset %>%
  pull(Diabetes) %>%
  createDataPartition(p = 0.7, list = FALSE)

train.data <- NHANES.subset %>%
  slice(train.indices)

test.data <- NHANES.subset %>%
  slice(-train.indices)



```

# Model building and hyperparameter tuning

## Model 1: Random Forest with 3 values of mtry and 3 values of ntree

```{r rf_model_building}

control.obj<-trainControl(method="cv", 
                          number=5, 
                          sampling="up", 
                          summaryFunction = twoClassSummary,
                          classProbs = TRUE)
#I use 5-fold cv here to reduce computational load
# The twoClassSummary function is designed for binary classification problems and will provide metrics like sensitivity, specificity, and Area Under the ROC Curve (AUC).
# ClassProbs=TRUE is necessary for twoClassSummary to work because it requires class probabilities to calculate AUC.

# hyperparameter tuning
# Try mtry of all, half of all, sqrt of all, 
# Try ntree of 100, 300, 500
feat.count<-c((ncol(train.data)-1), (ncol(train.data)-1)/2, sqrt(ncol(train.data)-1),2,3)
# I want to try more mtry, but it takes too long. Since the model have poor performace when mtry=1, I just add 2, 3 as candidate mtry.

grid.rf<-expand.grid(mtry=feat.count)

tree.num<-seq(100,500, by=200)

results.trees<-list()
for (ntree in tree.num){
  set.seed(123)
    rf.nhanes<-train(
                      Diabetes~., 
                      data=train.data, 
                      method="rf", 
                      trControl=control.obj, 
                      metric="ROC", 
                      tuneGrid=grid.rf, 
                      importance=TRUE, # the model will calculate variable importance measures
                      ntree=ntree)
    index<-toString(ntree)
  results.trees[[index]]<-rf.nhanes$results
}

plot(rf.nhanes)
output.nhanes<-bind_rows(results.trees, .id = "ntrees")
best.tune<-output.nhanes[which.max(output.nhanes[,"ROC"]),]
best.tune$mtry
# results.trees-no need to output
mtry.grid<-expand.grid(.mtry=best.tune$mtry)# choose the best tune to retrain the model

set.seed(123)
    rf.nhanes.bt<-train(
                      Diabetes~., 
                      data=train.data, 
                      method="rf", 
                      trControl=control.obj, 
                      metric="ROC", 
                      tuneGrid=mtry.grid, 
                      importance=TRUE,
                      ntree=as.numeric(best.tune$ntrees))


varImp(rf.nhanes.bt)
varImpPlot(rf.nhanes.bt$finalModel)



```

* Random forest have the function of feature selection, we might consider focusing on the most important variables for a more parsimonious model or for insight into the features most strongly associated with the outcome variable. Given the variable importance plot, some Education, race category is not that important(contribution <10%), but they can not be excluded separately.Ideally, I want to take a subset of features that contribute the most to model accuracy and purity to achieve the best trade-off between model complexity and predictive power.

* Instead of removing features,a more common approach would be to take the features until you reach a point of diminishing returns, where adding more features does not significantly increase the model's performance.

## Model 2: Support Vector Classifier

```{r svc_model_building}
set.seed(123)

control.obj<-trainControl(method="cv", 
                          number=5, 
                          sampling="up",
                          summaryFunction = twoClassSummary,
                          classProbs = TRUE)

#Repeat expanding the grid search
set.seed(123)

svc.nhanes<-train(
                  Diabetes ~ ., 
                  data=train.data, 
                  method="svmLinear", 
                  metric="ROC", 
                  trControl=control.obj, 
                  preProcess=c("center", "scale"), 
                  probability=TRUE, 
                  importance=TRUE,
                  tuneGrid=expand.grid(C=seq(0.0001,100, length=10)))
# for random forest, preProcess and probability is not necessary
# svc.nhanes$results
plot(svc.nhanes)

svc.nhanes.bt<-train(
                  Diabetes ~ ., 
                  data=train.data, 
                  method="svmLinear", 
                  metric="ROC", 
                  trControl=control.obj, 
                  preProcess=c("center", "scale"), 
                  probability=TRUE, 
                  importance=TRUE,
                  tuneGrid=data.frame(C=11.1112))



```

* At very low cost values (near 0), the model performance is poorer, with ROC values significantly lower than the rest of the cost values. This indicates underfitting;The ROC performance increase sharply as cost increases from 0 to 10; Beyond a certain cost threshold (around 10 to 20), the ROC value levels off and becomes relatively stable despite further increases in cost. This suggests that the model has reached a point where adding more complexity (lower regularization) does not significantly improve cross-validated performance. There is some variability in ROC scores at higher cost values, but it does not show a clear trend of improvement or degradation. This slight variation is typical in cross-validation results due to the randomness inherent in the partitioning of data. I would say the best tune might be 11.1112. 


## Model 3: Logistic Regression
```{r logit_model_building}
set.seed(123)

control.obj<-trainControl(method="cv", 
                          number=5, 
                          sampling="up",
                          summaryFunction = twoClassSummary,
                          classProbs = TRUE)

logit.nhanes<-train(
                    Diabetes~., 
                    data=train.data, 
                    method="glm", 
                    family="binomial",
                    metric="ROC", 
                    preProcess=c("center", "scale"), 
                    trControl=control.obj)
# importance=TRUE is not applicable in logistic regression

logit.nhanes$results
coef(logit.nhanes$finalModel)
# plot(logit.nhanes$finalModel,select=3) 
# The plot() function for a glm object can produce several types of diagnostic plots to assess the fit of the model. The select argument specifies which type of plot to produce:

# select=1: Residuals vs Fitted
# select=2: Normal Q-Q
# select=3: Scale-Location (also known as Spread-Location or Standardized residuals vs. Fitted)
# select=4: Cook's Distance plot
# select=5: Residuals vs Leverage plot that helps us to find influential cases
```

# Calibration

## Get predicted propabilities

```{r predicted_prop}

#Predict in test-set and output probabilities
rf.probs<-predict(rf.nhanes, test.data, type="prob")

#Pull out predicted probabilities for Diabetes=Yes
rf.pp<-rf.probs[,2]


svc.probs<-predict(svc.nhanes.bt,test.data, type="prob")
svc.pp<-svc.probs[,2]


#Predict in test-set using response type
logit.probs<-predict(logit.nhanes, test.data, type="prob")
logit.pp<-logit.probs[,2]

#Examine distributions of predicted probabilities
hist(rf.pp)
hist(svc.pp)
hist(logit.pp)
```

## Pre calibration plots
Plot and compare calibration curves across the three algorithms. 

```{r pre_calibration}
pred.prob<-data.frame(Class=test.data$Diabetes, logit=logit.pp, rf=rf.pp, svc=svc.pp)

calplot<-(calibration(Class ~ logit+rf+svc, data=pred.prob, class="Yes", cuts=10))

xyplot(calplot, auto.key=list(columns=3))
```

* The diagonal grey line represents ideal calibration, where the predicted probabilities match the observed frequencies. A model’s line above the diagonal indicates underconfidence (the model’s predictions are too conservative), and a line below the diagonal indicates overconfidence (the model’s predictions are too optimistic). In this case, rf and logit model shows similar calibration, they are both overconfident across all bins(when the predicted probability is high); The rf model is relatively well-calibrated, especially when the predicted probability is low.

## Calibrate the probabilities from SVC and RF/Post calibration plots

Partition testing data into 2 sets: set to train calibration and then set to evaluate results

Method 1: Platt's Scaling-train a logistic regression model on the outputs of your classifier


```{r partion_testing_data}

set.seed(123)
cal.data.index<-test.data$Diabetes %>% 
  createDataPartition(p=0.5, list=F)

cal.data<-test.data[cal.data.index, ]
final.test.data<-test.data[-cal.data.index, ]

```

1. Calibration of RF

```{r cali_rf}
#Predict on test-set without scaling to obtain raw pred prob in test set
rf.probs.nocal<-predict(rf.nhanes, final.test.data, type="prob")
rf.pp.nocal<-rf.probs.nocal[,2]

#Apply model developed on training data to calibration dataset to obtain predictions
rf.probs.cal<-predict(rf.nhanes, cal.data, type="prob")
rf.pp.cal<-rf.probs.cal[,2]

#Add to dataset with actual values from calibration data
calib.data.frame.rf<-data.frame(rf.pp.cal, cal.data$Diabetes)
colnames(calib.data.frame.rf)<-c("x", "y")

#Use logistic regression to model predicted probabilities from calibration data to actual vales
calib.model.rf<-glm(y ~ x, data=calib.data.frame.rf, family = binomial)

#Apply calibration model above to raw predicted probabilities from test set
data.test.rf<-data.frame(rf.pp.nocal)
colnames(data.test.rf)<-c("x")
platt.data.rf<-predict(calib.model.rf, data.test.rf, type="response")

platt.prob.rf<-data.frame(Class=final.test.data$Diabetes, rf.platt=platt.data.rf, rf=rf.pp.nocal)

calplot.rf<-(calibration(Class ~ rf.platt+rf, data=platt.prob.rf, class="Yes", cuts=10))
xyplot(calplot.rf, auto.key=list(columns=2))

hist(platt.data.rf)
```

* Platt's Scaling corrected the overconfident prediction when the bin midpoint < 60, improving the calibration of the RF model.

2. Calibration of SVC

```{r calibration_svc}
#Predict on test-set without scaling
svc.nocal<-predict(svc.nhanes.bt,final.test.data, type="prob")
svc.pp.nocal<-svc.nocal[,2]


#Apply model developed on training data to calibration dataset to obtain predictions
svc.cal<-predict(svc.nhanes.bt,cal.data, type="prob")
svc.pp.cal<-svc.cal[,2]

#Add to dataset with actual values from calibration data

calib.data.frame.svc<-data.frame(svc.pp.cal, cal.data$Diabetes)
colnames(calib.data.frame.svc)<-c("x", "y")
calib.model.svc<-glm(y ~ x, data=calib.data.frame.svc, family = binomial)

#Predict on test set using model developed in calibration
data.test.svc<-data.frame(svc.pp.nocal)
colnames(data.test.svc)<-c("x")
platt.data.svc<-predict(calib.model.svc, data.test.svc, type="response")

platt.prob.svc<-data.frame(Class=final.test.data$Diabetes, svc.platt=platt.data.svc, svc=svc.pp.nocal)

calplot.svc<-(calibration(Class ~ svc.platt+svc, data=platt.prob.svc, class="Yes", cuts=10))
xyplot(calplot.svc, auto.key=list(columns=2))
hist(platt.data.svc)
```

* Platt's Scaling corrected the overconfident prediction when the bin midpoint < 50, improving the calibration of the SVC model.

# Using resamples(multiple evaluation methods) to compare the three models

```{r resamples}
res = resamples(list(RF = rf.nhanes.bt,
                     SVC = svc.nhanes.bt,
                     GLM = logit.nhanes))
summary(res)
```

* I assume the purpose of this machine learning is to develop a tool to predict diabetes using above predictors. Estimated prevalence of diabetes in the United States is 11.6% in the population. So, we can just look at the calibration curve when the bin midpoint is low. Among the three models, rf had the best calibration at first. But after Platt's scaling, SVC can achieve better calibration than rf.

* AUC value, which indicates the discriminative ability is important. Meanwhile, for most practical applications in clinical setting, a balance between sensitivity and specificity is desirable. Given the results, ROC of the three models are similar, RF's specificity is too low whereas its sensitivity is higher than the other two models. So RF is not the optimal model. SVC had higher sensitivity, AUC compared with Logistic regression ,their specificity are the same. So, I would choose SVC as the optimal mode. 


